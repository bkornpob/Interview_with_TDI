{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Posterior Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "After this lecture, students will be able to:\n",
    "\n",
    "1. describe the difference between frequentist and bayesian inference\n",
    "\n",
    "2. describe the use of bayesian inference\n",
    "\n",
    "3. calculate conditional probability using Bayes' Rule\n",
    "\n",
    "4. derive posterior distribution\n",
    "\n",
    "5. perform hypothesis testing using Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lecture, we explore another paradigm of statistical inference called \"Bayesian.\" While the frequentist paradigm forms the inference by applying only the properties of the dataset, the Bayesian allows additional information to be integrated into the consideration. \n",
    "\n",
    "We start this lecture by discussing the Bayes' Rule, which is the fundamental concept for Bayesian inference. Then, we demonstrate how to apply the Bayes' Rule to form the Bayesian inference with an example with a finite and discrete space. After that, we generalize the concept to cover the case of infinite and continuous space.\n",
    "\n",
    "From the infinite and continuous example, we will see that most of the times we cannot construct the Bayesian inference due to the integration problem for the normalization factor. We can only analytically solve for the Bayesian inference with a specific case that carefully chooses a functional form as we call \"conjugate prior.\" One example of the conjugate prior, and the analytic Bayesian inference, is discussed.\n",
    "\n",
    "Then, we discuss how to apply a powerful computational algorithm called Markov Chain Monte Carlo (MCMC) to construct the Bayesian inference. Bayesian/MCMC pseudocode is discussed. A code example of Bayesian/MCMC is provided. Last, we demonstrate how to perform hypothesis testing using Bayesian inference.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This lecture assume students to be familiar with concepts of total probability, marginal probability, joint probability, and conditional probability. Knowledge about normal distribution and how to integrate a gaussian function is necessary when discussing the infinite and continuous Bayesian inference. Knowledge about python programming is required to understand the Bayesian/MCMC code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Bayes' Rule, as shown above, explains how the probability of observing an event A is updated by additional information B. For example, we have two boxes with balls. Box X has 8 red, and 2 white. The other Box Y has 5 red, and 5 white balls. We randomly pick one ball from an unknown box, and see it is a red. Then, we ask how much probability that we drew the red ball from Box X, i.e., P(X|red). \n",
    "\n",
    "Before the draw, we have 0.5 chance for drawing from any box, i.e., P(X) = P(Y) = 0.5. Once we draw a red, this probability changes. By intuition, we know that P(X|red) > 0.5 and P(Y|red) < 0.5 with 1 = P(X|red) + P(Y|red). And, if we think a bit deeper, we know that P(X|red) = 8/13 and P(Y|red) = 5/13, because we apply the only information we have (which is drawing a red ball) to form this probability. To be more precise, we know that we have 13 red balls in total: 8 balls in Box X and 5 balls in Box Y. This is exactly how Bayes' Rule works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be mathematically precise, we will try to derive the result by using the Bayes' Rule. We start with writing the rule out\n",
    "\n",
    "$$\n",
    "P(X|red) = \\frac{P(X \\cap red)}{P(red)}\n",
    "$$\n",
    "\n",
    "It would a lot easier if we were asked to find P(red|X), because we know P(red|X) = 8/10. Well, we can actually start from here because Bayes' Rule can also be expressed as\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A|B)P(B) = P(B|A)P(A)\n",
    "$$\n",
    "\n",
    "This identity is a direct result from the symmetry of a joint probability, i.e., $P(A \\cap B) = P(B \\cap A)$. So, we can find $P(X|red) = P(red|X)P(X) / P(red) = 0.8 \\times 0.5 / (13/20) = 8/13$. Walla! (Note that this calculation looks simple also because P(X) = P(Y), which will be relaxed in a later example). This Bayes' trick, as we would call, is the ground for understanding the Bayesian inference in this lecture. To summarize the Bayes' trick\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood, Prior, Posterior, and Normalization\n",
    "\n",
    "In this section, we introduce four technical names used in Bayesian inference. Associated with the previous example about boxes and balls, we would like to know the updated probability P(X|red) given additional information drawing red. Also, recall the Bayes' trick formula. We call the four terms in the Bayes' trick: likelihood, prior, posterior, and normalization.\n",
    "\n",
    "Posterior, as the name implied, is the updated probability P(X|red) that we would like to know after integrating additional information (drawing red) into consideration. Prior is the initial probability before acquiring additional information, i.e., P(X) = 0.5. Likelihood P(red|X) connects how the additional information provides the benefits of knowledge. And, the normalization at the denominator P(B), we can think of it as to normalize the scale back to the probability to be unity, i.e.,\n",
    "$\\sum_{\\forall i} P(A_i | B) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization: The Nuisance Parameter\n",
    "\n",
    "Typically, we adopt Bayesian approach when we have $A$ as the result of our interests, while $B$ as the information we have. Becasue of this, the posterior would be affected mainly by the two terms associated with $A$: the likelihood and prior. We can consider the normalization as a constant, that calibrates the total posterior to be unity as required in the probability scale. However, this constant is the nuisance parameter that is not actually in our interests, but can prevent us from finding an analytic solution of the problem in a simple manner.\n",
    "\n",
    "To show this, we get back to the previous boxes and balls example. In the previous setting of the example, we can calculate the posterior easily because $P(X) = P(Y) = 0.5$, which makes the normalization $P(red) = \\frac{13}{20}$ as the number of reds in X divided by the total of reds.\n",
    "\n",
    "Let's consider a new situation. The process starts with a box either X or Y randomly chosen with $P(X) = 0.3$. We do not know exactly which box was chosen, but we know the probability $P(X)$. We draw one ball from the chosen box to see its color (let's make it red again), and want to know $P(X|red)$.\n",
    "\n",
    "It is straightforwardly the same as the previous example that the likelihood $P(red|X) = 0.8$ and the prior $P(X) = 0.3$, but in the new example $P(red) \\neq \\frac{13}{20}$. To understand this, we invoke the total probability identity\n",
    "\n",
    "$$\n",
    "P(\\alpha) = \\sum_{\\forall i} P(\\alpha | \\beta_i) P(\\beta_i)\n",
    "$$\n",
    "\n",
    "The meaning of the expression is that the total probability of observing the event $\\alpha$ is composed of each individual probability of observing the event given a condition $\\beta_i$, $P(\\alpha | \\beta_i)$. And, we sum each individual probability because they are mutually exclusive. \n",
    "\n",
    "To make it in the context of our new example, \n",
    "\n",
    "$$\n",
    "P(red) = P(red | X) P(X) + P(red | Y) P(Y) = 0.8 \\times 0.3 + 0.5 \\times 0.7 = 0.59\n",
    "$$\n",
    "\n",
    "Therefore, $P(X | red) = 0.41$.\n",
    "\n",
    "Note that, we still can compute the numerical result of this new example becasue of the simplicity of finite discrete conditions $\\beta_i$. In general, if the condition is continuous in an interval [a,b] (which can be infinity),\n",
    "\n",
    "$$\n",
    "P(\\alpha) = \\int_{a}^{b} P(\\alpha | \\beta) P(\\beta) d\\beta\n",
    "$$\n",
    "\n",
    "It is because of this integration, in a complex situation we typically cannot find a closed form of this integration, which prevents us from finding the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytic Posterior by Choosing Conjugate Prior\n",
    "\n",
    "There is a way to find an analytic Bayesian solution by selecting the likelihood and prior functional forms that can be integrated analytically. Since the likelihood generally comes from underlying models or theories, this leaves us only the freedom to choose a suitable prior, which is called \"conjugate prior\" of the likelihood. \n",
    "\n",
    "In this section, we will go through one example of the conjugate prior. Let's say we know that a continuous random variable $x$ (extending to infinity) is identically and independently drawn from a $x \\sim N(\\theta,\\sigma^2)$ such that $\\theta$ is unknown but $\\sigma$ is known. Our intention is to observe one $x = X$, and characterize the posterior $P(\\theta | X)$.\n",
    "\n",
    "Recall the normal distribution. For this example, we can express the likelihood as\n",
    "\n",
    "$$\n",
    "P(X | \\theta) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left[-\\frac{(x - \\theta)^2}{2 \\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "In this example, our only unknown is $\\theta$. We would like to select a conjugate prior $P(\\theta)$ that will give us $\\int_{-\\infty}^{\\infty} P(X | \\theta) P(\\theta) d\\theta$ integrable. One possible conjugate is alsoa gaussian function itself. To show this, we choose the prior $P(\\theta)$ such that $\\theta \\sim N(\\mu_0,\\sigma_{0}^2)$. (Note that both $\\mu_0$ and $\\sigma_{0}$ are known because this is prior). This will be left as an exercise to show that\n",
    "\n",
    "$$\n",
    "P(X | \\theta) P(\\theta) = \\exp \\left[ -\\frac{(\\theta - \\mu_1)^2}{2 \\sigma_{1}^2} \\right]\n",
    "$$\n",
    "\n",
    "By applying $\\int_{-\\infty}^{\\infty} \\exp[ -x^2 ] dx = \\sqrt{\\pi}$, we get $\\int_{-\\infty}^{\\infty} P(X | \\theta) P(\\theta) d\\theta = \\sqrt{2 \\pi \\sigma_{1}^2}$. Therefore, the posterior $P(\\theta | X)$ is also a normal distribution with mean $\\mu_1$ and variance $\\sigma_{1}^2$ where\n",
    "\n",
    "$$\n",
    "\\mu_1 = \\sigma_{1}^2 \\left( \\frac{X}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_{0}^2} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_{1}^2 = \\left( \\frac{1}{\\sigma^2} + \\frac{1}{\\sigma_{0}^2} \\right)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Posterior by Markov Chain Monte Carlo\n",
    "\n",
    "As we can see from the previous section that it is not always possible to find an analytic Bayesian posterior if we cannot find a conjugate prior of a given likelihood. Moreover, in practice we most likely prefer a certain functional form or property of prior regarding to other information such as theories or results from previous research. Therefore, we actually do not have much of the freedom to choose any prior, and the Bayesian posterior cannot be expressed in a closed form.\n",
    "\n",
    "Fortunately, the Markov Chain Monte Carlo (MCMC) simulation is a method that can help us computationally characterize the posterior. Without getting into details, the pseudocode for the Bayesian/MCMC is as following:\n",
    "\n",
    "0. Collect the data $X$\n",
    "\n",
    "1. Set up a prior $P(\\theta)$, and likelihood $P(X|\\theta)$.\n",
    "\n",
    "2. Initialize the parameter $\\theta = \\theta_0$.\n",
    "\n",
    "3. Propose a new $\\theta = \\theta_1$ with a given rule $q(\\theta_1 | \\theta_0)$ (which will be discussed later)\n",
    "\n",
    "4. Compute $$\n",
    "\\alpha = \\frac{P(X|\\theta_1)P(\\theta_1) / q(\\theta_1 | \\theta_0)}{P(X|\\theta_0)P(\\theta_0) / q(\\theta_0 | \\theta_1)}\n",
    "$$\n",
    "\n",
    "5. Determine whether to accept $\\theta_1$ with the acceptance rate $\\alpha$\n",
    "\n",
    "6. If accept, record $\\theta_1$ and set $\\theta_0 = \\theta_1$. If not, record $\\theta_0$ and leave $\\theta_0 = \\theta_0$\n",
    "\n",
    "7. Repeat steps 3 -- 6 until we get a satisfied result (which will be discussed later)\n",
    "\n",
    "8. Discard some early iterations before the convergence of $\\theta$ (which will be discussed later)\n",
    "\n",
    "9. The leftover array of $\\theta$ represents the posterior, which can be visualized by a histogram\n",
    "\n",
    "Note: step 3 -- 6 is called \"Metropolis-Hastings\" algorithm, which is a MCMC random sampling algorithm for one unknown posterior parameter. If the problem has more than one unknown posterior parameters, a better MCMC sampling algorithm than the Metropolis is called \"Gibbs sampling.\"\n",
    "\n",
    "## Rule for proposing a new  position $q(\\theta_1 | \\theta_0)$\n",
    "\n",
    "This step is the core of MCMC. The idea of MCMC is to perform a random move to a new position from the current position. In our case, the current position is $\\theta_0$, and we want to move to a new position $\\theta_1$. There are several ways to determine where to move as long as the move is random with a given rule $q(\\theta_1 | \\theta_0)$. For example, we might propose the move with a distance $\\epsilon \\sim N(0,1)$ such that $\\theta_1 = \\theta_0 + \\epsilon$. Therefore, $q(\\theta_1 | \\theta_0) \\sim N(\\theta_0,1)$ in this case. \n",
    "\n",
    "Since a new position is practically drawn from a symmetric function such that $q(\\theta_1 | \\theta_0) = q(\\theta_0 | \\theta_1)$ (e.g., the normal distribution), the two $q$ terms in the $\\alpha$ are cancelled out. Therefore, the acceptance rate $\\alpha$ is the odd ratio between the posterior probability at the new position relative to at the current one, which is more intuitive than when having to consider the two $q$ terms. In the following discussion, we will apply a symmetric rule so that we can discard the impacts of $q$ safely. However, it is worth to keep in mind that the $q$ terms must be taken into account when the symmetry is broken. Intuitively, the term $q$ makes the probability like $P(X|\\theta_1)P(\\theta_1) / q(\\theta_1 | \\theta_0)$ represent an effective probability of the posterior probability. \n",
    "\n",
    "## When to stop the iteration?\n",
    "\n",
    "MCMC is an iterative simulation, therefore we have to know when to stop. In this case, let's imagine that the initial $\\theta_0$ is a value which is very far away from the \"perfect\" $\\theta$. In each iteration, $\\theta$ should effectively move closer to the perfect value. Then, $\\theta$ will converge to the perfect value, but fluctuate slightly around due to the random walk in each iteration. \n",
    "\n",
    "As mentioned in step 8, we discard some early iteration before the convergence of $\\theta$. There are no best method to determine when the Markov Chain converges. Typically, the chain plot, and potential scale reduction factor (a.k.a. PSRF) are computed to determine the likelihood of having a converge chain. In the following example, only the chain plot will be introduced.\n",
    "\n",
    "After, we identify when the chain starts to converge, we continue the iteration and keep recording all the $\\theta$s afterwards. The more iterations we run after the convergence, the better estimate of the posterior is. Therefore, we actually stop when we think that the computed posterior would be accurate enough for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Example of Bayesian/MCMC\n",
    "\n",
    "In this section, a simple Bayesian/MCMC code is presented. We are trying to solve the same problem as presented with the normal-distributed likelihood and normal-distributed conjugate prior, so that we can compare the final result with the analytic solution. Note also that we consider only the case of having only one observation, and we work with a log scale to prevent the computational crash due to a very large/small number due to the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1234) # set the random state\n",
    "\n",
    "def log_gauss(x, mu, sig):\n",
    "    a = np.sqrt(2 * 3.14 * sig**2)\n",
    "    b = -1. * (x - mu)**2 / (2. * sig**2)\n",
    "    return b - np.log(a)\n",
    "\n",
    "def rule_newPosition(mu,sig):\n",
    "    return np.random.normal(mu,sig)\n",
    "\n",
    "def post_params(x, sigma, mu0, sigma0):\n",
    "    sigma1 = np.sqrt((sigma**-2 + sigma0**-2)**-1)\n",
    "    mu1 = (sigma1**2) * (x/sigma**2 + mu0/sigma0**2)\n",
    "    return (mu1,sigma1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from the simulation:\n",
      "count    99000.000000\n",
      "mean        19.617536\n",
      "std          0.982833\n",
      "min         15.096871\n",
      "25%         18.947265\n",
      "50%         19.625415\n",
      "75%         20.289686\n",
      "max         23.851541\n",
      "dtype: float64\n",
      "\n",
      "Analytic $\\mu_1$ = 19.615, $\\sigma_1$ = 0.981\n"
     ]
    }
   ],
   "source": [
    "record = [] # for recording\n",
    "iter = 100000 # number of iterations\n",
    "\n",
    "# 0. Collect the data $X$\n",
    "X = 10.\n",
    "\n",
    "# 1. Set up a prior $P(\\theta)$, and likelihood $P(X|\\theta)$.\n",
    "theta0,sigma = None,5. # likelihood\n",
    "mu0,sigma0 = 20.,1. # prior\n",
    "\n",
    "# 2. Initialize the parameter $\\theta = \\theta_0$.\n",
    "theta0 = 0.\n",
    "\n",
    "for i in np.arange(iter):\n",
    "    # 3. Propose a new $\\theta = \\theta_1$ with a given rule $q(\\theta_1 | \\theta_0)$ (which will be discussed later)\n",
    "    theta1 = theta0 + rule_newPosition(0.,5.)\n",
    "\n",
    "    # 4. Compute $$\n",
    "    # \\alpha = \\frac{P(X|\\theta_1)P(\\theta_1) / q(\\theta_1 | \\theta_0)}{P(X|\\theta_0)P(\\theta_0) / q(\\theta_0 | \\theta_1)}\n",
    "    # $$\n",
    "    a = log_gauss(X,theta1,sigma) + log_gauss(theta1,mu0,sigma0)\n",
    "    b = log_gauss(X,theta0,sigma) + log_gauss(theta0,mu0,sigma0)\n",
    "    log_alpha = a - b\n",
    "    alpha = np.exp(log_alpha)\n",
    "\n",
    "    # 5. Determine whether to accept $\\theta_1$ with the acceptance rate $\\alpha$\n",
    "    c = np.random.uniform()\n",
    "    if c < alpha:\n",
    "        accept = True\n",
    "    else:\n",
    "        accept = False\n",
    "\n",
    "    # 6. If accept, record $\\theta_1$ and set $\\theta_0 = \\theta_1$. If not, record $\\theta_0$ and leave $\\theta_0 = \\theta_0$\n",
    "    if accept:\n",
    "        theta0 = theta1\n",
    "    record.append(theta0)\n",
    "\n",
    "    # 7. Repeat steps 3 -- 6 until we get a satisfied result (which will be discussed later)\n",
    "\n",
    "# 8. Discard some early iterations before the convergence of $\\theta$ (which will be discussed later)\n",
    "plt.figure()\n",
    "plt.plot(record)\n",
    "plt.show()\n",
    "record = record[1000:]\n",
    "\n",
    "# 9. The leftover array of $\\theta$ represents the posterior, which can be visualized by a histogram\n",
    "plt.figure()\n",
    "plt.hist(record,density=True)\n",
    "plt.show()\n",
    "\n",
    "print('Result from the simulation:')\n",
    "print(pd.Series(record).describe())\n",
    "\n",
    "a = post_params(X,sigma,mu0,sigma0)\n",
    "print('\\nAnalytic $\\mu_1$ = {0:.3f}, $\\sigma_1$ = {1:.3f}'.format(a[0],a[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first plot is the chain plot showing some early moves that were trying to converge to the solution, and we discarded that section as shown in step 8. Also, the final result from the simulation (mean, std) is approximately the same as we expect from the analytic solution ($\\mu_1$, $\\sigma_1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "In this last section, we discuss how to perform a hypothesis testing using Bayesian inference. We continue this discussion with by simulated result of recorded $\\theta$ from the previous section.\n",
    "\n",
    "We start with setting up a hypothesis. Let's say, we would like to know whether $\\theta$ is indifference to 19.\n",
    "$$\n",
    "H_0: \\theta = 19\n",
    "$$\n",
    "$$\n",
    "H_1: \\theta \\neq 19\n",
    "$$\n",
    "\n",
    "Next, since we already have the distribution (i.e., the histogram), we can proceed by choosing the significance level. Let's take the significance level $\\alpha = 0.05$.\n",
    "\n",
    "Then, we can either proceed by constructing a confidence interval (CI) around the recorded mean {{'{0:.3f}'.format(pd.Series(record).mean())}}, or find the critical values to contruct the rejection regions. We continue with constructing the rejection regions, which is less complicated than the other method.\n",
    "\n",
    "Since our hypothesis is 2-tailed, we would like to find the critical values on the left and right ($\\theta_{crit,L},\\theta_{crit,R})$ that makes $P(\\theta \\leq \\theta_{crit,L}) = P(\\theta > \\theta_{crit,R}) = 0.025$, where we will reject the null hypothesis if 19 is inside these regions. We find the values by using the function describe() in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    99000.000000\n",
       "mean        19.617536\n",
       "std          0.982833\n",
       "min         15.096871\n",
       "2.5%        17.688489\n",
       "50%         19.625415\n",
       "97.5%       21.511654\n",
       "max         23.851541\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(record).describe(percentiles=[0.025,1-0.025])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we have $\\theta_{crit,L} = 17.69$ and $\\theta_{crit,R} = 21.51$. Since 19 is not inside the rejection regions, we conclude that \"we fail to reject the null hypothesis at 0.05 significance level.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We discuss the Bayesian posterior inference in this lecture. The Bayesian inference is different to the frequentist one in particularly that the Bayesian inference allows extra information, other than the properties of the sample, to be integrated as the prior when forming the posterior. The extra information is assumed to be dependent to the event of interests connected by the likelihood. \n",
    "\n",
    "To construct the Bayesian posterior inference, three components are required: likelihood, prior, and normalization. Even though the normalization does not directly affect the posterior, this component is necessary in order to completely construct the inference. However, because the term involves with an integration, only a carefully chosen conjugate prior can result in a closed-form analytic solution. In general, the term cannot be integrated, and Bayesian/MCMC is the computational method that helps us constructing the inference computationally. The pseudocode, and a code example of Bayesian/MCMC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
